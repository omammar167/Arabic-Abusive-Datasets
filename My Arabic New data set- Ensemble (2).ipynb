{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import itertools\n",
    "import csv,codecs,nltk,re\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, NuSVC , LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 241: expected 2 fields, saw 3\\n'\n"
     ]
    }
   ],
   "source": [
    "# -----  New Dataset ------------\n",
    "data = pd.read_csv(r\"D:\\Cyber_2021-1.csv\" ,sep=\";\",error_bad_lines=False)\n",
    "data.columns = [\"text\",\"Label\"]\n",
    "#data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----  Hamdy Dataset ------------\n",
    "data = pd.read_csv(r\"D:\\Dataset\\Test\\Unbalanced\\Arabic\\Hamdy\\Hamdy-Twitter.csv\" ,sep=\";\",encoding=\"utf-8\")\n",
    "data.columns = [\"text\",\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----  Al akroot Dataset ------------\n",
    "data = pd.read_csv(r\"D:\\Dataset\\Test\\Unbalanced\\Arabic\\Alakrot\\Alakrot-Youtube.csv\" ,sep=\";\",encoding=\"utf-8\")\n",
    "data.columns = [\"text\",\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13249"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مبروك و سامحونا لعجزنا التام. عقبال اللي جوه. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>كلنا بره ومش هنبطل نزايد على العجايز الي جابون...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بدل ما انت قاعد بره كده تعالي ازرع الصحرا</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>قذر اتفووو ماتيجى مصر وتورينا نفسك كدا ياجبان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وهكذا رجال الشو اللي محرومين من عمل برنامج الغ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Label\n",
       "0  مبروك و سامحونا لعجزنا التام. عقبال اللي جوه. ...      0\n",
       "1  كلنا بره ومش هنبطل نزايد على العجايز الي جابون...      1\n",
       "2          بدل ما انت قاعد بره كده تعالي ازرع الصحرا      0\n",
       "3      قذر اتفووو ماتيجى مصر وتورينا نفسك كدا ياجبان      1\n",
       "4  وهكذا رجال الشو اللي محرومين من عمل برنامج الغ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    6859\n",
       "0     6355\n",
       "0B       1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data.Label !='0B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text preprocessing (Arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"_\", \" \", text)\n",
    "    text = re.sub(\"#\", \" \", text)\n",
    "    \n",
    "    text = re.sub(\"ؤ\", \"و\", text)\n",
    "    #text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    return(text)\n",
    "def stopwordremoval(text):\n",
    "    stop=stopwords.words(\"arabic\")\n",
    "    needed_words=[]\n",
    "    words=word_tokenize(text)\n",
    "    for w in words:\n",
    "         if len(w)>=2 and w not in stop:\n",
    "                needed_words.append(w)\n",
    "    filterd_sent= \" \".join(needed_words)\n",
    "    return filterd_sent\n",
    "def removenonarabic(text):\n",
    "    n=re.sub(r'[a-zA-Z?]', '', text).strip()\n",
    "    n=re.sub('\\W+',' ', n )\n",
    "    n=re.sub('_','', n )\n",
    "    n = ''.join([i for i in n if not i.isdigit()])\n",
    "    return n\n",
    "def stemming(text):\n",
    "    st = ISRIStemmer()\n",
    "    stemmed_words=[]\n",
    "    words=word_tokenize(text)\n",
    "    for w in words:\n",
    "        stemmed_words.append(st.stem(w))\n",
    "    stemmed_sent=\" \".join(stemmed_words)\n",
    "    return stemmed_sent\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "def preparedatasets(data):\n",
    "    sentences=[]\n",
    "    for index,r in data.iterrows():\n",
    "        text=normalize(r['text'])\n",
    "        text=stopwordremoval(text)\n",
    "        text=removenonarabic(text)\n",
    "        #text=stemming(text)\n",
    "        sentences.append([text,r['Label']])\n",
    "    df_sentence=DataFrame(sentences, columns=[\"text\", \"Label\"])\n",
    "    return df_sentence        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=preparedatasets(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=preparedatasets(data)\n",
    "data.dropna(inplace=True)\n",
    "data.head()\n",
    "data['word_count'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "#Remove 0 and 1 word_count posts\n",
    "new_data=data[(data.word_count >1)]\n",
    "new_data.describe()    \n",
    "data=new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop_duplicates( keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    637\n",
       "0    450\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of words in the dataset\n",
    "def get_count(data):\n",
    "    d2=[]\n",
    "    tt=\"\"\n",
    "    for index,r in data.iterrows():\n",
    "        d2.append((r['text'] ))\n",
    "        tt+=r['text']\n",
    "    return len(tt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(data['text'], data['Label'],test_size=0.20, random_state = 0)\n",
    "vectorizer = TfidfVectorizer( analyzer='word',smooth_idf=True, ngram_range=(1,2))\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vectorized = vectorizer.transform(X_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoteBook\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7018348623853211\n",
      "f1_score 0.7014814518453775\n",
      "recall 0.7018348623853211\n",
      "precision 0.7011578126439656\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.60      0.61        83\n",
      "           1       0.76      0.76      0.76       135\n",
      "\n",
      "    accuracy                           0.70       218\n",
      "   macro avg       0.68      0.68      0.68       218\n",
      "weighted avg       0.70      0.70      0.70       218\n",
      "\n",
      " confussion matrix:\n",
      " [[ 50  33]\n",
      " [ 32 103]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoteBook\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ---------- Voting ---- \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = KNeighborsClassifier()\n",
    "model2= DecisionTreeClassifier()\n",
    "model3=LogisticRegression()\n",
    "model4=LinearSVC() \n",
    "\n",
    "model1.fit(X_train_vectorized,y_train)\n",
    "model2.fit(X_train_vectorized,y_train)\n",
    "model3.fit(X_train_vectorized,y_train)\n",
    "model4.fit(X_train_vectorized,y_train)\n",
    "\n",
    "model = VotingClassifier(estimators=[('dt', model1),('rf', model2),('ml', model3),('lsv', model4)], voting='hard')\n",
    "model.fit(X_train_vectorized,y_train)\n",
    "model.score(vectorizer.transform(X_test),y_test)\n",
    "\n",
    "predictions = model.predict(vectorizer.transform(X_test))\n",
    "print(\"accuracy\",accuracy_score(y_test, predictions))\n",
    "print(\"f1_score\",f1_score(y_test, predictions,average='weighted'))\n",
    "print(\"recall\",recall_score(y_test, predictions,average='weighted'))\n",
    "print(\"precision\",precision_score(y_test, predictions,average='weighted'))    \n",
    "print ('\\n clasification report:\\n', classification_report(y_test, predictions))\n",
    "print (' confussion matrix:\\n',confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9601873536299765\n",
      "f1_score 0.9601869896976261\n",
      "recall 0.9601873536299765\n",
      "precision 0.9601971586286903\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.96      0.96      1283\n",
      "           0       0.96      0.96      0.96      1279\n",
      "\n",
      "    accuracy                           0.96      2562\n",
      "   macro avg       0.96      0.96      0.96      2562\n",
      "weighted avg       0.96      0.96      0.96      2562\n",
      "\n",
      " confussion matrix:\n",
      " [[1235   48]\n",
      " [  54 1225]]\n"
     ]
    }
   ],
   "source": [
    "# --- ---------- Bagging ----------------\n",
    "# 1- Desicion Tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "model.score(vectorizer.transform(X_test),y_test)\n",
    "\n",
    "predictions = model.predict(vectorizer.transform(X_test))\n",
    "print(\"accuracy\",accuracy_score(y_test, predictions))\n",
    "print(\"f1_score\",f1_score(y_test, predictions,average='weighted'))\n",
    "print(\"recall\",recall_score(y_test, predictions,average='weighted'))\n",
    "print(\"precision\",precision_score(y_test, predictions,average='weighted'))    \n",
    "print ('\\n clasification report:\\n', classification_report(y_test, predictions))\n",
    "print (' confussion matrix:\\n',confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoteBook\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trees used :  100\n",
      "\n",
      "accuracy_score on train dataset :  0.9988290398126464\n",
      "\n",
      "accuracy_score on test dataset :  0.9613583138173302\n",
      "accuracy 0.9613583138173302\n",
      "f1_score 0.9613580488967537\n",
      "recall 0.9613583138173302\n",
      "precision 0.9613651073205672\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.96      0.96      1283\n",
      "           0       0.96      0.96      0.96      1279\n",
      "\n",
      "    accuracy                           0.96      2562\n",
      "   macro avg       0.96      0.96      0.96      2562\n",
      "weighted avg       0.96      0.96      0.96      2562\n",
      "\n",
      " confussion matrix:\n",
      " [[1236   47]\n",
      " [  52 1227]]\n"
     ]
    }
   ],
   "source": [
    " # 2- ------  Random- forest -----\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# number of trees used\n",
    "print('Number of Trees used : ', 100)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(X_train_vectorized)\n",
    "#print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(y_train,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(vectorizer.transform(X_test))\n",
    "#print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(y_test,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)\n",
    "\n",
    "\n",
    "print(\"accuracy\",accuracy_score(y_test, predict_test))\n",
    "print(\"f1_score\",f1_score(y_test, predict_test,average='weighted'))\n",
    "print(\"recall\",recall_score(y_test, predict_test,average='weighted'))\n",
    "print(\"precision\",precision_score(y_test, predict_test,average='weighted'))    \n",
    "print ('\\n clasification report:\\n', classification_report(y_test, predict_test))\n",
    "print (' confussion matrix:\\n',confusion_matrix(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9480874316939891\n",
      "f1_score 0.9480617555879797\n",
      "recall 0.9480874316939891\n",
      "precision 0.9490435937254006\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.93      0.95      1283\n",
      "           0       0.93      0.97      0.95      1279\n",
      "\n",
      "    accuracy                           0.95      2562\n",
      "   macro avg       0.95      0.95      0.95      2562\n",
      "weighted avg       0.95      0.95      0.95      2562\n",
      "\n",
      " confussion matrix:\n",
      " [[1187   96]\n",
      " [  37 1242]]\n"
     ]
    }
   ],
   "source": [
    "# ----------  Booosting -------\n",
    "#----Boosting -- AdaBoost \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "model.score(vectorizer.transform(X_test),y_test)\n",
    "\n",
    "predictions = model.predict(vectorizer.transform(X_test))\n",
    "print(\"accuracy\",accuracy_score(y_test, predictions))\n",
    "print(\"f1_score\",f1_score(y_test, predictions,average='weighted'))\n",
    "print(\"recall\",recall_score(y_test, predictions,average='weighted'))\n",
    "print(\"precision\",precision_score(y_test, predictions,average='weighted'))    \n",
    "print ('\\n clasification report:\\n', classification_report(y_test, predictions))\n",
    "print (' confussion matrix:\\n',confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7997658079625293\n",
      "f1_score 0.792640221338046\n",
      "recall 0.7997658079625293\n",
      "precision 0.848091038112197\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.61      0.75      1283\n",
      "           0       0.72      0.99      0.83      1279\n",
      "\n",
      "    accuracy                           0.80      2562\n",
      "   macro avg       0.85      0.80      0.79      2562\n",
      "weighted avg       0.85      0.80      0.79      2562\n",
      "\n",
      " confussion matrix:\n",
      " [[ 788  495]\n",
      " [  18 1261]]\n"
     ]
    }
   ],
   "source": [
    "#----Boosting -- GradientBoost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "model.score(vectorizer.transform(X_test),y_test)\n",
    "\n",
    "predictions = model.predict(vectorizer.transform(X_test))\n",
    "print(\"accuracy\",accuracy_score(y_test, predictions))\n",
    "print(\"f1_score\",f1_score(y_test, predictions,average='weighted'))\n",
    "print(\"recall\",recall_score(y_test, predictions,average='weighted'))\n",
    "print(\"precision\",precision_score(y_test, predictions,average='weighted'))    \n",
    "print ('\\n clasification report:\\n', classification_report(y_test, predictions))\n",
    "print (' confussion matrix:\\n',confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoteBook\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:39:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "accuracy 0.8645589383294301\n",
      "f1_score 0.8626891565103247\n",
      "recall 0.8645589383294301\n",
      "precision 0.8858993569343891\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.75      0.85      1283\n",
      "           0       0.79      0.98      0.88      1279\n",
      "\n",
      "    accuracy                           0.86      2562\n",
      "   macro avg       0.89      0.86      0.86      2562\n",
      "weighted avg       0.89      0.86      0.86      2562\n",
      "\n",
      " confussion matrix:\n",
      " [[ 959  324]\n",
      " [  23 1256]]\n"
     ]
    }
   ],
   "source": [
    "#----Boosting -- XGBoost\n",
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "model.score(vectorizer.transform(X_test),y_test)\n",
    "\n",
    "predictions = model.predict(vectorizer.transform(X_test))\n",
    "print(\"accuracy\",accuracy_score(y_test, predictions))\n",
    "print(\"f1_score\",f1_score(y_test, predictions,average='weighted'))\n",
    "print(\"recall\",recall_score(y_test, predictions,average='weighted'))\n",
    "print(\"precision\",precision_score(y_test, predictions,average='weighted'))    \n",
    "print ('\\n clasification report:\\n', classification_report(y_test, predictions))\n",
    "print (' confussion matrix:\\n',confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
